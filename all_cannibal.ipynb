{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be35f348-c18f-4af5-ba28-88779f70a94f",
   "metadata": {},
   "source": [
    "# Get API Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a99dd-676a-476a-a3c6-98a3fdd84fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set your service account information\n",
    "SERVICE_ACCOUNT_FILE = 'example.json'\n",
    "SCOPES = ['https://www.googleapis.com/auth/webmasters.readonly']\n",
    "\n",
    "# Set your site URL\n",
    "SITE_URL = \"https://example.com/\"\n",
    "\n",
    "# Set the start and end dates\n",
    "start_date = datetime(2024, 2, 1)\n",
    "end_date = datetime(2024, 8, 7)\n",
    "\n",
    "# Maximum number of rows per API request\n",
    "maxRows = 25000\n",
    "\n",
    "# Create a list to store the output data\n",
    "output_rows = []\n",
    "\n",
    "def main():\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "\n",
    "    service = build('searchconsole', 'v1', credentials=credentials)\n",
    "\n",
    "    # Iterate through the date range\n",
    "    for date in date_range(start_date, end_date):\n",
    "        date_str = date.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"Fetching data for {date_str}...\")\n",
    "\n",
    "        i = 0\n",
    "        while True:\n",
    "            request = {\n",
    "                'startDate': date_str,\n",
    "                'endDate': date_str,\n",
    "                'dimensions': [\"query\", \"page\", \"country\", \"device\"],\n",
    "                \"searchType\": \"Web\",\n",
    "                'rowLimit': maxRows,\n",
    "                'startRow': i * maxRows\n",
    "            }\n",
    "\n",
    "            response = service.searchanalytics().query(siteUrl=SITE_URL, body=request).execute()\n",
    "\n",
    "            if 'rows' not in response:\n",
    "                print(f\"No data received for {date_str}. Continuing...\")\n",
    "                break\n",
    "\n",
    "            for row in response['rows']:\n",
    "                keyword = row['keys'][0]\n",
    "                page = row['keys'][1]\n",
    "                country = row['keys'][2]\n",
    "                device = row['keys'][3]\n",
    "                output_row = [date_str, keyword, page, country, device, row['clicks'], row['impressions'],\n",
    "                              row['ctr'],\n",
    "                              row['position']]\n",
    "                output_rows.append(output_row)\n",
    "\n",
    "            if len(response['rows']) < maxRows:\n",
    "                # Reached the last page, move on to the next day\n",
    "                break\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    # Process the data (e.g., write to a CSV file)\n",
    "    process_data(output_rows)\n",
    "\n",
    "def date_range(start, end):\n",
    "    for n in range(int((end - start).days) + 1):\n",
    "        yield start + timedelta(n)\n",
    "\n",
    "def process_data(data):\n",
    "    import csv\n",
    "    with open('search_console_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(\n",
    "            [\"Date\", \"Keyword\", \"Page\", \"Country\", \"Device\", \"Clicks\", \"Impressions\", \"CTR\", \"Position\"])\n",
    "        writer.writerows(data)  # Write the raw data directly\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f89158-b0c0-4f8f-9839-4c79b7e6f2f0",
   "metadata": {},
   "source": [
    "# average_gun Calculate\n",
    "#### (The ratio of the number of days the URL received impressions to the total number of days from the date of the first impression to the current date: average_gun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f85303-2fe5-4935-b89c-b4804f2eecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the data set\n",
    "df = pd.read_csv(\"search_console_data.csv\")\n",
    "\n",
    "# Convert the Date column to datetime object\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Empty lists for the results DataFrame\n",
    "results = []\n",
    "\n",
    "# Perform calculations for each URL\n",
    "for url in df['Page'].unique():\n",
    "    # Filter data for the URL\n",
    "    url_data = df[df['Page'] == url].copy()\n",
    "\n",
    "    # Find the first impression date\n",
    "    ilk_gosterim_tarihi = url_data[url_data['Position'] > 0]['Date'].min()\n",
    "\n",
    "    # Calculate the number of days passed since the first impression date until 06.08.2024\n",
    "    toplam_gun = (datetime(2024, 8, 7) - ilk_gosterim_tarihi).days + 1             #CHANGE THIS WITH CURRENT DATE ðŸ‘ˆðŸ‘ˆðŸ‘ˆ\n",
    "\n",
    "    # Filter data from the first impression date to 06.08.2024\n",
    "    url_data = url_data[url_data['Date'] >= ilk_gosterim_tarihi]\n",
    "\n",
    "    # Find the days with impressions (use unique() to count only once)\n",
    "    gosterim_gunleri = url_data[url_data['Position'] > 0]['Date'].unique()\n",
    "    gosterim_gun_sayisi = len(gosterim_gunleri)\n",
    "\n",
    "    # Calculate the average impression day\n",
    "    average_gun = gosterim_gun_sayisi / toplam_gun\n",
    "\n",
    "    # Calculate the average of Position values (only those greater than 0)\n",
    "    ortalama_position = url_data[url_data['Position'] > 0]['Position'].mean()\n",
    "\n",
    "    # Calculate the square of differences from the average for each day\n",
    "    kare_farklar = [(p - ortalama_position)**2 for p in url_data[url_data['Position'] > 0]['Position']]\n",
    "\n",
    "    # Calculate the square root of the average of squared differences (standard deviation)\n",
    "    standart_sapma = (sum(kare_farklar) / len(kare_farklar))**0.5\n",
    "\n",
    "    # Calculate the coefficient of variation\n",
    "    degisim_kaysayÄ±sÄ± = (standart_sapma / ortalama_position) * 100 if ortalama_position != 0 else 0\n",
    "\n",
    "    # Add the results to the list\n",
    "    results.append([url, average_gun, gosterim_gun_sayisi, toplam_gun, degisim_kaysayÄ±sÄ±])\n",
    "\n",
    "# Create the results DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['URL', 'average_gun', 'gosterim_gun_sayisi', 'toplam_gun', 'degisim_kaysayÄ±sÄ±'])\n",
    "\n",
    "results_df['average_gun'] = results_df['average_gun'].round(2)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\"sonuclarFINAL2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae400aba-3d31-44c1-bf57-4e6f99cc9ac6",
   "metadata": {},
   "source": [
    "# Grouping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f02f7-a703-4afa-9e52-d7b82b314e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def group_and_aggregate_data(csv_file_path):\n",
    "    \"\"\"\n",
    "    Groups data by URL, combines same keywords for the same URL,\n",
    "    and sums the click and impression values.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): Path to the CSV file to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Grouped and aggregated data.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "\n",
    "    grouped_data = df.groupby('Page').agg(\n",
    "        Keywords=('Keyword', lambda x: '|'.join(set(x))),  # Combine unique keywords with '|'\n",
    "        TotalClicks=('Clicks', 'sum'),\n",
    "        TotalImpressions=('Impressions', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "def write_grouped_data_to_csv(grouped_data, output_file_path):\n",
    "    \"\"\"\n",
    "    Writes the grouped and aggregated data to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        grouped_data (pd.DataFrame): Grouped data.\n",
    "        output_file_path (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    grouped_data.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file_path = 'search_console_data.csv'  # Path to the input CSV file\n",
    "    output_file_path = 'grouped_search_console_data.csv'  # Path to the output CSV file\n",
    "\n",
    "    grouped_data = group_and_aggregate_data(csv_file_path)\n",
    "    write_grouped_data_to_csv(grouped_data, output_file_path)\n",
    "    print(f\"Grouped data written to '{output_file_path}' file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea70e6-3f8a-4913-a82f-027b7298282f",
   "metadata": {},
   "source": [
    "# Calculating similarity between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d65e4-81b9-49e1-a39a-6e41bab0e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"grouped_search_console_data.csv\")\n",
    "\n",
    "def create_keyword_sets(row):\n",
    "  \"\"\"Creates a set of keywords from the 'Keywords' column in each row by splitting with '|'.\n",
    "     Excludes words specified with regex.\"\"\"\n",
    "  keywords = row['Keywords'].split('|')\n",
    "  # Excludes words specified with regex\n",
    "  excluded_pattern = r\"YOUR REGEX\"  \n",
    "  keywords = [word for word in keywords if not re.search(excluded_pattern, word)]\n",
    "  \n",
    "  return set(keywords)\n",
    "\n",
    "# Converts the keywords in the 'Keywords' column of each row to a set\n",
    "df['Keyword Sets'] = df.apply(create_keyword_sets, axis=1)\n",
    "\n",
    "# Adds all Page values to a list\n",
    "pages = df['Page'].tolist()\n",
    "\n",
    "# Compares the keyword sets of each page and calculates the similarity percentage\n",
    "similarity_results = []\n",
    "\n",
    "for i in range(len(pages)):\n",
    "  for j in range(i+1, len(pages)):\n",
    "    page1_keywords = df['Keyword Sets'][i]\n",
    "    page2_keywords = df['Keyword Sets'][j]\n",
    "\n",
    "    common_keywords = page1_keywords.intersection(page2_keywords)\n",
    "    # Sets similarity to 0 if len(page1_keywords) is 0\n",
    "    similarity = 0 if len(page1_keywords) == 0 else len(common_keywords) / len(page1_keywords) * 100\n",
    "\n",
    "    # Prints the similarity percentage as a decimal\n",
    "    similarity_results.append({\n",
    "        'Page 1': pages[i],\n",
    "        'Page 2': pages[j],\n",
    "        'Similarity (%)': int(similarity)\n",
    "    })\n",
    "\n",
    "# Converts the results to a DataFrame\n",
    "similarity_df = pd.DataFrame(similarity_results)\n",
    "\n",
    "# Prints the DataFrame\n",
    "print(similarity_df)\n",
    "\n",
    "similarity_df.to_csv(\"final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
