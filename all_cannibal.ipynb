{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be35f348-c18f-4af5-ba28-88779f70a94f",
   "metadata": {},
   "source": [
    "# API Verilerini Alma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a99dd-676a-476a-a3c6-98a3fdd84fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Service Account bilgilerinizi ayarlayın\n",
    "SERVICE_ACCOUNT_FILE = 'ck-health-indexing-api-dc7562c3e0e0.json'\n",
    "SCOPES = ['https://www.googleapis.com/auth/webmasters.readonly']\n",
    "\n",
    "# Site URL'nizi ayarlayın\n",
    "SITE_URL = \"https://ckhealthturkey.com/\"\n",
    "\n",
    "# Başlangıç ve bitiş tarihlerini ayarlayın\n",
    "start_date = datetime(2024, 2, 1)\n",
    "end_date = datetime(2024, 8, 7)\n",
    "\n",
    "# API istekleri için maksimum satır sayısı\n",
    "maxRows = 25000\n",
    "\n",
    "# Çıktı verilerini saklamak için bir liste oluşturun\n",
    "output_rows = []\n",
    "\n",
    "def main():\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "\n",
    "    service = build('searchconsole', 'v1', credentials=credentials)\n",
    "\n",
    "    # Tarih aralığını döngüye alın\n",
    "    for date in date_range(start_date, end_date):\n",
    "        date_str = date.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"Veriler {date_str} tarihi için çekiliyor...\")\n",
    "\n",
    "        i = 0\n",
    "        while True:\n",
    "            request = {\n",
    "                'startDate': date_str,\n",
    "                'endDate': date_str,\n",
    "                'dimensions': [\"query\", \"page\", \"country\", \"device\"],\n",
    "                \"searchType\": \"Web\",\n",
    "                'rowLimit': maxRows,\n",
    "                'startRow': i * maxRows\n",
    "            }\n",
    "\n",
    "            response = service.searchanalytics().query(siteUrl=SITE_URL, body=request).execute()\n",
    "\n",
    "            if 'rows' not in response:\n",
    "                print(f\"{date_str} tarihindeki veriler için yanıt alınamadı. Devam ediliyor...\")\n",
    "                break\n",
    "\n",
    "            for row in response['rows']:\n",
    "                keyword = row['keys'][0]\n",
    "                page = row['keys'][1]\n",
    "                country = row['keys'][2]\n",
    "                device = row['keys'][3]\n",
    "                output_row = [date_str, keyword, page, country, device, row['clicks'], row['impressions'],\n",
    "                              row['ctr'],\n",
    "                              row['position']]\n",
    "                output_rows.append(output_row)\n",
    "\n",
    "            if len(response['rows']) < maxRows:\n",
    "                # Son sayfaya ulaşıldı, bir sonraki güne geç\n",
    "                break\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    # Verileri işleyin (örneğin, CSV dosyasına yazdırın)\n",
    "    process_data(output_rows)\n",
    "\n",
    "def date_range(start, end):\n",
    "    for n in range(int((end - start).days) + 1):\n",
    "        yield start + timedelta(n)\n",
    "\n",
    "def process_data(data):\n",
    "    import csv\n",
    "    with open('search_console_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(\n",
    "            [\"Date\", \"Keyword\", \"Page\", \"Country\", \"Device\", \"Clicks\", \"Impressions\", \"CTR\", \"Position\"])\n",
    "        writer.writerows(data)  # Ham veriyi doğrudan yazdır\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f89158-b0c0-4f8f-9839-4c79b7e6f2f0",
   "metadata": {},
   "source": [
    "# average_gun Hesaplama \n",
    "#### (URL'nin toplam gösterim aldığı gün sayısının ilk gösterim aldığı tarihten güncel tarihe kadarki süreye oranı: average_gun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f85303-2fe5-4935-b89c-b4804f2eecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Veri setini yükle\n",
    "df = pd.read_csv(\"search_console_data.csv\")\n",
    "\n",
    "# Tarih sütununu datetime objesine dönüştür\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Sonuç DataFrame'i için boş listeler\n",
    "results = []\n",
    "\n",
    "# Her URL için hesaplamalar yap\n",
    "for url in df['Page'].unique():\n",
    "    # URL'ye ait verileri filtrele\n",
    "    url_data = df[df['Page'] == url].copy()\n",
    "\n",
    "    # İlk gösterim tarihini bul\n",
    "    ilk_gosterim_tarihi = url_data[url_data['Position'] > 0]['Date'].min()\n",
    "\n",
    "    # İlk gösterim tarihinden 06.08.2024'e kadar geçen gün sayısını hesapla\n",
    "    toplam_gun = (datetime(2024, 8, 7) - ilk_gosterim_tarihi).days + 1             #BURAYI GÜNCEL TARİHLE DEĞİŞTİRİN 👈👈👈\n",
    "\n",
    "    # İlk gösterim tarihinden 06.08.2024'e kadar olan verileri filtrele\n",
    "    url_data = url_data[url_data['Date'] >= ilk_gosterim_tarihi]\n",
    "\n",
    "    # Gösterim alınan günleri bul (sadece bir kez saymak için unique() kullan)\n",
    "    gosterim_gunleri = url_data[url_data['Position'] > 0]['Date'].unique()\n",
    "    gosterim_gun_sayisi = len(gosterim_gunleri)\n",
    "\n",
    "    # Ortalama gösterim gününü hesapla\n",
    "    average_gun = gosterim_gun_sayisi / toplam_gun\n",
    "\n",
    "    # Position değerlerinin ortalamasını hesapla (sadece 0'dan büyük olanlar)\n",
    "    ortalama_position = url_data[url_data['Position'] > 0]['Position'].mean()\n",
    "\n",
    "    # Her gün için ortalamadan farkların karesini hesapla\n",
    "    kare_farklar = [(p - ortalama_position)**2 for p in url_data[url_data['Position'] > 0]['Position']]\n",
    "\n",
    "    # Kare farkların ortalamasının karekökünü hesapla (standart sapma)\n",
    "    standart_sapma = (sum(kare_farklar) / len(kare_farklar))**0.5\n",
    "\n",
    "    # Varyasyon katsayısını hesapla\n",
    "    degisim_kaysayısı = (standart_sapma / ortalama_position) * 100 if ortalama_position != 0 else 0\n",
    "\n",
    "    # Sonuçları listeye ekle\n",
    "    results.append([url, average_gun, gosterim_gun_sayisi, toplam_gun, degisim_kaysayısı])\n",
    "\n",
    "# Sonuç DataFrame'ini oluştur\n",
    "results_df = pd.DataFrame(results, columns=['URL', 'average_gun', 'gosterim_gun_sayisi', 'toplam_gun', 'degisim_kaysayısı'])\n",
    "\n",
    "results_df['average_gun'] = results_df['average_gun'].round(2)\n",
    "\n",
    "# Sonuçları CSV dosyasına kaydet\n",
    "results_df.to_csv(\"sonuclarFINAL2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae400aba-3d31-44c1-bf57-4e6f99cc9ac6",
   "metadata": {},
   "source": [
    "# Gruplama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f02f7-a703-4afa-9e52-d7b82b314e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def group_and_aggregate_data(csv_file_path):\n",
    "    \"\"\"\n",
    "    Verileri URL'lere göre gruplandırır, aynı URL için aynı kelimeleri birleştirir\n",
    "    ve tıklama ve gösterim değerlerini toplar.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): İşlenecek CSV dosyasının yolu.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Gruplandırılmış ve toplanmış veriler.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "\n",
    "    grouped_data = df.groupby('Page').agg(\n",
    "        Keywords=('Keyword', lambda x: '|'.join(set(x))),  # Eşsiz keyword'leri '|' ile birleştir\n",
    "        TotalClicks=('Clicks', 'sum'),\n",
    "        TotalImpressions=('Impressions', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "def write_grouped_data_to_csv(grouped_data, output_file_path):\n",
    "    \"\"\"\n",
    "    Gruplandırılmış ve toplanmış verileri CSV dosyasına yazar.\n",
    "\n",
    "    Args:\n",
    "        grouped_data (pd.DataFrame): Gruplandırılmış veriler.\n",
    "        output_file_path (str): Çıkış CSV dosyasının yolu.\n",
    "    \"\"\"\n",
    "\n",
    "    grouped_data.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file_path = 'search_console_data.csv'  # Giriş CSV dosyasının yolu\n",
    "    output_file_path = 'grouped_search_console_data.csv'  # Çıkış CSV dosyasının yolu\n",
    "\n",
    "    grouped_data = group_and_aggregate_data(csv_file_path)\n",
    "    write_grouped_data_to_csv(grouped_data, output_file_path)\n",
    "    print(f\"Gruplandırılmış veriler '{output_file_path}' dosyasına yazıldı.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea70e6-3f8a-4913-a82f-027b7298282f",
   "metadata": {},
   "source": [
    "# Kümeler arası benzerlik hesaplama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d65e4-81b9-49e1-a39a-6e41bab0e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"grouped_search_console_data.csv\")\n",
    "\n",
    "def create_keyword_sets(row):\n",
    "  \"\"\"Her satırdaki 'Keywords' sütunundaki kelimeleri | ile bölerek bir küme oluşturur.\n",
    "     Regex ile belirtilen kelimeleri hariç tutar.\"\"\"\n",
    "  keywords = row['Keywords'].split('|')\n",
    "  # Regex ile belirtilen kelimeleri hariç tutar\n",
    "  excluded_pattern = r\"^ck.*|\\bck\\b|ckhealth\"  \n",
    "  keywords = [word for word in keywords if not re.search(excluded_pattern, word)]\n",
    "  \n",
    "  return set(keywords)\n",
    "\n",
    "# Her satırdaki 'Keywords' sütunundaki kelimeleri bir kümeye dönüştürür\n",
    "df['Keyword Sets'] = df.apply(create_keyword_sets, axis=1)\n",
    "\n",
    "# Tüm Page değerlerini bir listeye atar\n",
    "pages = df['Page'].tolist()\n",
    "\n",
    "# Her sayfanın kelime kümesini karşılaştırır ve benzerlik yüzdesini hesaplar\n",
    "similarity_results = []\n",
    "\n",
    "for i in range(len(pages)):\n",
    "  for j in range(i+1, len(pages)):\n",
    "    page1_keywords = df['Keyword Sets'][i]\n",
    "    page2_keywords = df['Keyword Sets'][j]\n",
    "\n",
    "    common_keywords = page1_keywords.intersection(page2_keywords)\n",
    "    #  len(page1_keywords) değeri 0 ise similarity'i 0 olarak ayarlar\n",
    "    similarity = 0 if len(page1_keywords) == 0 else len(common_keywords) / len(page1_keywords) * 100\n",
    "\n",
    "    # Benzerlik yüzdesini ondalıklı olarak yazdırır\n",
    "    similarity_results.append({\n",
    "        'Page 1': pages[i],\n",
    "        'Page 2': pages[j],\n",
    "        'Similarity (%)': int(similarity)\n",
    "    })\n",
    "\n",
    "# Sonuçları bir DataFrame'e dönüştürür\n",
    "similarity_df = pd.DataFrame(similarity_results)\n",
    "\n",
    "# DataFrame'i yazdırır\n",
    "print(similarity_df)\n",
    "\n",
    "similarity_df.to_csv(\"kümelemeFINAL.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
