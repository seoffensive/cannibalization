{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be35f348-c18f-4af5-ba28-88779f70a94f",
   "metadata": {},
   "source": [
    "# API Verilerini Alma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a99dd-676a-476a-a3c6-98a3fdd84fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Service Account bilgilerinizi ayarlayÄ±n\n",
    "SERVICE_ACCOUNT_FILE = 'ck-health-indexing-api-dc7562c3e0e0.json'\n",
    "SCOPES = ['https://www.googleapis.com/auth/webmasters.readonly']\n",
    "\n",
    "# Site URL'nizi ayarlayÄ±n\n",
    "SITE_URL = \"https://ckhealthturkey.com/\"\n",
    "\n",
    "# BaÅŸlangÄ±Ã§ ve bitiÅŸ tarihlerini ayarlayÄ±n\n",
    "start_date = datetime(2024, 2, 1)\n",
    "end_date = datetime(2024, 8, 7)\n",
    "\n",
    "# API istekleri iÃ§in maksimum satÄ±r sayÄ±sÄ±\n",
    "maxRows = 25000\n",
    "\n",
    "# Ã‡Ä±ktÄ± verilerini saklamak iÃ§in bir liste oluÅŸturun\n",
    "output_rows = []\n",
    "\n",
    "def main():\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "\n",
    "    service = build('searchconsole', 'v1', credentials=credentials)\n",
    "\n",
    "    # Tarih aralÄ±ÄŸÄ±nÄ± dÃ¶ngÃ¼ye alÄ±n\n",
    "    for date in date_range(start_date, end_date):\n",
    "        date_str = date.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"Veriler {date_str} tarihi iÃ§in Ã§ekiliyor...\")\n",
    "\n",
    "        i = 0\n",
    "        while True:\n",
    "            request = {\n",
    "                'startDate': date_str,\n",
    "                'endDate': date_str,\n",
    "                'dimensions': [\"query\", \"page\", \"country\", \"device\"],\n",
    "                \"searchType\": \"Web\",\n",
    "                'rowLimit': maxRows,\n",
    "                'startRow': i * maxRows\n",
    "            }\n",
    "\n",
    "            response = service.searchanalytics().query(siteUrl=SITE_URL, body=request).execute()\n",
    "\n",
    "            if 'rows' not in response:\n",
    "                print(f\"{date_str} tarihindeki veriler iÃ§in yanÄ±t alÄ±namadÄ±. Devam ediliyor...\")\n",
    "                break\n",
    "\n",
    "            for row in response['rows']:\n",
    "                keyword = row['keys'][0]\n",
    "                page = row['keys'][1]\n",
    "                country = row['keys'][2]\n",
    "                device = row['keys'][3]\n",
    "                output_row = [date_str, keyword, page, country, device, row['clicks'], row['impressions'],\n",
    "                              row['ctr'],\n",
    "                              row['position']]\n",
    "                output_rows.append(output_row)\n",
    "\n",
    "            if len(response['rows']) < maxRows:\n",
    "                # Son sayfaya ulaÅŸÄ±ldÄ±, bir sonraki gÃ¼ne geÃ§\n",
    "                break\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    # Verileri iÅŸleyin (Ã¶rneÄŸin, CSV dosyasÄ±na yazdÄ±rÄ±n)\n",
    "    process_data(output_rows)\n",
    "\n",
    "def date_range(start, end):\n",
    "    for n in range(int((end - start).days) + 1):\n",
    "        yield start + timedelta(n)\n",
    "\n",
    "def process_data(data):\n",
    "    import csv\n",
    "    with open('search_console_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(\n",
    "            [\"Date\", \"Keyword\", \"Page\", \"Country\", \"Device\", \"Clicks\", \"Impressions\", \"CTR\", \"Position\"])\n",
    "        writer.writerows(data)  # Ham veriyi doÄŸrudan yazdÄ±r\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f89158-b0c0-4f8f-9839-4c79b7e6f2f0",
   "metadata": {},
   "source": [
    "# average_gun Hesaplama \n",
    "#### (URL'nin toplam gÃ¶sterim aldÄ±ÄŸÄ± gÃ¼n sayÄ±sÄ±nÄ±n ilk gÃ¶sterim aldÄ±ÄŸÄ± tarihten gÃ¼ncel tarihe kadarki sÃ¼reye oranÄ±: average_gun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f85303-2fe5-4935-b89c-b4804f2eecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Veri setini yÃ¼kle\n",
    "df = pd.read_csv(\"search_console_data.csv\")\n",
    "\n",
    "# Tarih sÃ¼tununu datetime objesine dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# SonuÃ§ DataFrame'i iÃ§in boÅŸ listeler\n",
    "results = []\n",
    "\n",
    "# Her URL iÃ§in hesaplamalar yap\n",
    "for url in df['Page'].unique():\n",
    "    # URL'ye ait verileri filtrele\n",
    "    url_data = df[df['Page'] == url].copy()\n",
    "\n",
    "    # Ä°lk gÃ¶sterim tarihini bul\n",
    "    ilk_gosterim_tarihi = url_data[url_data['Position'] > 0]['Date'].min()\n",
    "\n",
    "    # Ä°lk gÃ¶sterim tarihinden 06.08.2024'e kadar geÃ§en gÃ¼n sayÄ±sÄ±nÄ± hesapla\n",
    "    toplam_gun = (datetime(2024, 8, 7) - ilk_gosterim_tarihi).days + 1             #BURAYI GÃœNCEL TARÄ°HLE DEÄÄ°ÅTÄ°RÄ°N ğŸ‘ˆğŸ‘ˆğŸ‘ˆ\n",
    "\n",
    "    # Ä°lk gÃ¶sterim tarihinden 06.08.2024'e kadar olan verileri filtrele\n",
    "    url_data = url_data[url_data['Date'] >= ilk_gosterim_tarihi]\n",
    "\n",
    "    # GÃ¶sterim alÄ±nan gÃ¼nleri bul (sadece bir kez saymak iÃ§in unique() kullan)\n",
    "    gosterim_gunleri = url_data[url_data['Position'] > 0]['Date'].unique()\n",
    "    gosterim_gun_sayisi = len(gosterim_gunleri)\n",
    "\n",
    "    # Ortalama gÃ¶sterim gÃ¼nÃ¼nÃ¼ hesapla\n",
    "    average_gun = gosterim_gun_sayisi / toplam_gun\n",
    "\n",
    "    # Position deÄŸerlerinin ortalamasÄ±nÄ± hesapla (sadece 0'dan bÃ¼yÃ¼k olanlar)\n",
    "    ortalama_position = url_data[url_data['Position'] > 0]['Position'].mean()\n",
    "\n",
    "    # Her gÃ¼n iÃ§in ortalamadan farklarÄ±n karesini hesapla\n",
    "    kare_farklar = [(p - ortalama_position)**2 for p in url_data[url_data['Position'] > 0]['Position']]\n",
    "\n",
    "    # Kare farklarÄ±n ortalamasÄ±nÄ±n karekÃ¶kÃ¼nÃ¼ hesapla (standart sapma)\n",
    "    standart_sapma = (sum(kare_farklar) / len(kare_farklar))**0.5\n",
    "\n",
    "    # Varyasyon katsayÄ±sÄ±nÄ± hesapla\n",
    "    degisim_kaysayÄ±sÄ± = (standart_sapma / ortalama_position) * 100 if ortalama_position != 0 else 0\n",
    "\n",
    "    # SonuÃ§larÄ± listeye ekle\n",
    "    results.append([url, average_gun, gosterim_gun_sayisi, toplam_gun, degisim_kaysayÄ±sÄ±])\n",
    "\n",
    "# SonuÃ§ DataFrame'ini oluÅŸtur\n",
    "results_df = pd.DataFrame(results, columns=['URL', 'average_gun', 'gosterim_gun_sayisi', 'toplam_gun', 'degisim_kaysayÄ±sÄ±'])\n",
    "\n",
    "results_df['average_gun'] = results_df['average_gun'].round(2)\n",
    "\n",
    "# SonuÃ§larÄ± CSV dosyasÄ±na kaydet\n",
    "results_df.to_csv(\"sonuclarFINAL2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae400aba-3d31-44c1-bf57-4e6f99cc9ac6",
   "metadata": {},
   "source": [
    "# Gruplama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f02f7-a703-4afa-9e52-d7b82b314e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def group_and_aggregate_data(csv_file_path):\n",
    "    \"\"\"\n",
    "    Verileri URL'lere gÃ¶re gruplandÄ±rÄ±r, aynÄ± URL iÃ§in aynÄ± kelimeleri birleÅŸtirir\n",
    "    ve tÄ±klama ve gÃ¶sterim deÄŸerlerini toplar.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): Ä°ÅŸlenecek CSV dosyasÄ±nÄ±n yolu.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: GruplandÄ±rÄ±lmÄ±ÅŸ ve toplanmÄ±ÅŸ veriler.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "\n",
    "    grouped_data = df.groupby('Page').agg(\n",
    "        Keywords=('Keyword', lambda x: '|'.join(set(x))),  # EÅŸsiz keyword'leri '|' ile birleÅŸtir\n",
    "        TotalClicks=('Clicks', 'sum'),\n",
    "        TotalImpressions=('Impressions', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "def write_grouped_data_to_csv(grouped_data, output_file_path):\n",
    "    \"\"\"\n",
    "    GruplandÄ±rÄ±lmÄ±ÅŸ ve toplanmÄ±ÅŸ verileri CSV dosyasÄ±na yazar.\n",
    "\n",
    "    Args:\n",
    "        grouped_data (pd.DataFrame): GruplandÄ±rÄ±lmÄ±ÅŸ veriler.\n",
    "        output_file_path (str): Ã‡Ä±kÄ±ÅŸ CSV dosyasÄ±nÄ±n yolu.\n",
    "    \"\"\"\n",
    "\n",
    "    grouped_data.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file_path = 'search_console_data.csv'  # GiriÅŸ CSV dosyasÄ±nÄ±n yolu\n",
    "    output_file_path = 'grouped_search_console_data.csv'  # Ã‡Ä±kÄ±ÅŸ CSV dosyasÄ±nÄ±n yolu\n",
    "\n",
    "    grouped_data = group_and_aggregate_data(csv_file_path)\n",
    "    write_grouped_data_to_csv(grouped_data, output_file_path)\n",
    "    print(f\"GruplandÄ±rÄ±lmÄ±ÅŸ veriler '{output_file_path}' dosyasÄ±na yazÄ±ldÄ±.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea70e6-3f8a-4913-a82f-027b7298282f",
   "metadata": {},
   "source": [
    "# KÃ¼meler arasÄ± benzerlik hesaplama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d65e4-81b9-49e1-a39a-6e41bab0e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"grouped_search_console_data.csv\")\n",
    "\n",
    "def create_keyword_sets(row):\n",
    "  \"\"\"Her satÄ±rdaki 'Keywords' sÃ¼tunundaki kelimeleri | ile bÃ¶lerek bir kÃ¼me oluÅŸturur.\n",
    "     Regex ile belirtilen kelimeleri hariÃ§ tutar.\"\"\"\n",
    "  keywords = row['Keywords'].split('|')\n",
    "  # Regex ile belirtilen kelimeleri hariÃ§ tutar\n",
    "  excluded_pattern = r\"^ck.*|\\bck\\b|ckhealth\"  \n",
    "  keywords = [word for word in keywords if not re.search(excluded_pattern, word)]\n",
    "  \n",
    "  return set(keywords)\n",
    "\n",
    "# Her satÄ±rdaki 'Keywords' sÃ¼tunundaki kelimeleri bir kÃ¼meye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r\n",
    "df['Keyword Sets'] = df.apply(create_keyword_sets, axis=1)\n",
    "\n",
    "# TÃ¼m Page deÄŸerlerini bir listeye atar\n",
    "pages = df['Page'].tolist()\n",
    "\n",
    "# Her sayfanÄ±n kelime kÃ¼mesini karÅŸÄ±laÅŸtÄ±rÄ±r ve benzerlik yÃ¼zdesini hesaplar\n",
    "similarity_results = []\n",
    "\n",
    "for i in range(len(pages)):\n",
    "  for j in range(i+1, len(pages)):\n",
    "    page1_keywords = df['Keyword Sets'][i]\n",
    "    page2_keywords = df['Keyword Sets'][j]\n",
    "\n",
    "    common_keywords = page1_keywords.intersection(page2_keywords)\n",
    "    #  len(page1_keywords) deÄŸeri 0 ise similarity'i 0 olarak ayarlar\n",
    "    similarity = 0 if len(page1_keywords) == 0 else len(common_keywords) / len(page1_keywords) * 100\n",
    "\n",
    "    # Benzerlik yÃ¼zdesini ondalÄ±klÄ± olarak yazdÄ±rÄ±r\n",
    "    similarity_results.append({\n",
    "        'Page 1': pages[i],\n",
    "        'Page 2': pages[j],\n",
    "        'Similarity (%)': int(similarity)\n",
    "    })\n",
    "\n",
    "# SonuÃ§larÄ± bir DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r\n",
    "similarity_df = pd.DataFrame(similarity_results)\n",
    "\n",
    "# DataFrame'i yazdÄ±rÄ±r\n",
    "print(similarity_df)\n",
    "\n",
    "similarity_df.to_csv(\"kÃ¼melemeFINAL.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
