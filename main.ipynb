{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be35f348-c18f-4af5-ba28-88779f70a94f",
   "metadata": {},
   "source": [
    "# Get API Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a99dd-676a-476a-a3c6-98a3fdd84fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set your service account information\n",
    "SERVICE_ACCOUNT_FILE = 'example.json'\n",
    "SCOPES = ['https://www.googleapis.com/auth/webmasters.readonly']\n",
    "\n",
    "# Set your site URL\n",
    "SITE_URL = \"https://example.com/\"\n",
    "\n",
    "# Set the start and end dates\n",
    "start_date = datetime(2024, 2, 1)\n",
    "end_date = datetime(2024, 8, 7)\n",
    "\n",
    "# Maximum number of rows per API request\n",
    "maxRows = 25000\n",
    "\n",
    "# Create a list to store the output data\n",
    "output_rows = []\n",
    "\n",
    "def main():\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "\n",
    "    service = build('searchconsole', 'v1', credentials=credentials)\n",
    "\n",
    "    # Iterate through the date range\n",
    "    for date in date_range(start_date, end_date):\n",
    "        date_str = date.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"Fetching data for {date_str}...\")\n",
    "\n",
    "        i = 0\n",
    "        while True:\n",
    "            request = {\n",
    "                'startDate': date_str,\n",
    "                'endDate': date_str,\n",
    "                'dimensions': [\"query\", \"page\", \"country\", \"device\"],\n",
    "                \"searchType\": \"Web\",\n",
    "                'rowLimit': maxRows,\n",
    "                'startRow': i * maxRows\n",
    "            }\n",
    "\n",
    "            response = service.searchanalytics().query(siteUrl=SITE_URL, body=request).execute()\n",
    "\n",
    "            if 'rows' not in response:\n",
    "                print(f\"No data received for {date_str}. Continuing...\")\n",
    "                break\n",
    "\n",
    "            for row in response['rows']:\n",
    "                keyword = row['keys'][0]\n",
    "                page = row['keys'][1]\n",
    "                country = row['keys'][2]\n",
    "                device = row['keys'][3]\n",
    "                output_row = [date_str, keyword, page, country, device, row['clicks'], row['impressions'],\n",
    "                              row['ctr'],\n",
    "                              row['position']]\n",
    "                output_rows.append(output_row)\n",
    "\n",
    "            if len(response['rows']) < maxRows:\n",
    "                # Reached the last page, move on to the next day\n",
    "                break\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    # Process the data (e.g., write to a CSV file)\n",
    "    process_data(output_rows)\n",
    "\n",
    "def date_range(start, end):\n",
    "    for n in range(int((end - start).days) + 1):\n",
    "        yield start + timedelta(n)\n",
    "\n",
    "def process_data(data):\n",
    "    import csv\n",
    "    with open('search_console_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(\n",
    "            [\"Date\", \"Keyword\", \"Page\", \"Country\", \"Device\", \"Clicks\", \"Impressions\", \"CTR\", \"Position\"])\n",
    "        writer.writerows(data)  # Write the raw data directly\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f89158-b0c0-4f8f-9839-4c79b7e6f2f0",
   "metadata": {},
   "source": [
    "# average_gun Calculate\n",
    "#### When canonicalizing URLs, it provides additional data to help decide which URL to canonicalize. (The ratio of the number of days the URL received impressions to the total number of days from the date of the first impression to the current date: average_gun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f85303-2fe5-4935-b89c-b4804f2eecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the data set\n",
    "df = pd.read_csv(\"search_console_data.csv\")\n",
    "\n",
    "# Convert the Date column to datetime object\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Empty lists for the results DataFrame\n",
    "results = []\n",
    "\n",
    "# Perform calculations for each URL\n",
    "for url in df['Page'].unique():\n",
    "    # Filter data for the URL\n",
    "    url_data = df[df['Page'] == url].copy()\n",
    "\n",
    "    # Find the first impression date\n",
    "    ilk_gosterim_tarihi = url_data[url_data['Position'] > 0]['Date'].min()\n",
    "\n",
    "    # Calculate the number of days passed since the first impression date until 06.08.2024\n",
    "    toplam_gun = (datetime(2024, 8, 7) - ilk_gosterim_tarihi).days + 1             #CHANGE THIS WITH CURRENT DATE ðŸ‘ˆðŸ‘ˆðŸ‘ˆ\n",
    "\n",
    "    # Filter data from the first impression date to 06.08.2024\n",
    "    url_data = url_data[url_data['Date'] >= ilk_gosterim_tarihi]\n",
    "\n",
    "    # Find the days with impressions (use unique() to count only once)\n",
    "    gosterim_gunleri = url_data[url_data['Position'] > 0]['Date'].unique()\n",
    "    gosterim_gun_sayisi = len(gosterim_gunleri)\n",
    "\n",
    "    # Calculate the average impression day\n",
    "    average_gun = gosterim_gun_sayisi / toplam_gun\n",
    "\n",
    "    # Calculate the average of Position values (only those greater than 0)\n",
    "    ortalama_position = url_data[url_data['Position'] > 0]['Position'].mean()\n",
    "\n",
    "    # Calculate the square of differences from the average for each day\n",
    "    kare_farklar = [(p - ortalama_position)**2 for p in url_data[url_data['Position'] > 0]['Position']]\n",
    "\n",
    "    # Calculate the square root of the average of squared differences (standard deviation)\n",
    "    standart_sapma = (sum(kare_farklar) / len(kare_farklar))**0.5\n",
    "\n",
    "    # Calculate the coefficient of variation\n",
    "    degisim_kaysayÄ±sÄ± = (standart_sapma / ortalama_position) * 100 if ortalama_position != 0 else 0\n",
    "\n",
    "    # Add the results to the list\n",
    "    results.append([url, average_gun, gosterim_gun_sayisi, toplam_gun, degisim_kaysayÄ±sÄ±])\n",
    "\n",
    "# Create the results DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['URL', 'average_gun', 'gosterim_gun_sayisi', 'toplam_gun', 'degisim_kaysayÄ±sÄ±'])\n",
    "\n",
    "results_df['average_gun'] = results_df['average_gun'].round(2)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\"sonuclarFINAL2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae400aba-3d31-44c1-bf57-4e6f99cc9ac6",
   "metadata": {},
   "source": [
    "# Grouping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f02f7-a703-4afa-9e52-d7b82b314e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def group_and_aggregate_data(csv_file_path):\n",
    "    \"\"\"\n",
    "    Groups data by URL, combines same keywords for the same URL,\n",
    "    and sums the click and impression values.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): Path to the CSV file to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Grouped and aggregated data.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "    \n",
    "    df['Include'] = df.groupby(['Page', 'Keyword'])['Clicks'].transform('sum') > 0\n",
    "    \n",
    "    df_filtered = df[df['Include']].copy()\n",
    "\n",
    "    grouped_data = df_filtered.groupby('Page').agg(\n",
    "        Keywords=('Keyword', lambda x: '|'.join(set(x))),  # Combine unique keywords with '|'\n",
    "        TotalClicks=('Clicks', 'sum'),\n",
    "        TotalImpressions=('Impressions', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "def write_grouped_data_to_csv(grouped_data, output_file_path):\n",
    "    \"\"\"\n",
    "    Writes the grouped and aggregated data to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        grouped_data (pd.DataFrame): Grouped data.\n",
    "        output_file_path (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    grouped_data.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file_path = 'search_console_data.csv'  # Path to the input CSV file\n",
    "    output_file_path = 'grouped_search_console_data.csv'  # Path to the output CSV file\n",
    "\n",
    "    grouped_data = group_and_aggregate_data(csv_file_path)\n",
    "    write_grouped_data_to_csv(grouped_data, output_file_path)\n",
    "    print(f\"Grouped data written to '{output_file_path}' file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea70e6-3f8a-4913-a82f-027b7298282f",
   "metadata": {},
   "source": [
    "# Calculating similarity between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d65e4-81b9-49e1-a39a-6e41bab0e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv(\"grouped_search_console_data.csv\")\n",
    "search_console_df = pd.read_csv(\"search_console_data.csv\")\n",
    "\n",
    "def create_keyword_sets(row):\n",
    "    keywords = row['Keywords'].split('|')\n",
    "    excluded_pattern = r\"YOUR REGEX\"    # Excludes words specified with regex\n",
    "    keywords = [word for word in keywords if not re.search(excluded_pattern, word)]\n",
    "    return set(keywords)\n",
    "\n",
    "df['Keyword Sets'] = df.apply(create_keyword_sets, axis=1)\n",
    "pages = df['Page'].tolist()\n",
    "\n",
    "similarity_results = []\n",
    "total_iterations = (len(pages) * (len(pages) - 1)) // 2\n",
    "\n",
    "with tqdm(total=total_iterations, desc=\"Smilarity Calculating\") as pbar:\n",
    "    for i in range(len(pages)):\n",
    "        for j in range(i + 1, len(pages)):\n",
    "            page1_keywords = df['Keyword Sets'][i]\n",
    "            page2_keywords = df['Keyword Sets'][j]\n",
    "\n",
    "            str_page1_keywords = ' '.join(page1_keywords)\n",
    "            str_page2_keywords = ' '.join(page2_keywords)\n",
    "\n",
    "            similarity_sort = fuzz.token_sort_ratio(str_page1_keywords, str_page2_keywords)\n",
    "            similarity_set = fuzz.token_set_ratio(str_page1_keywords, str_page2_keywords)\n",
    "\n",
    "            #fuzzywuzzy smilarity\n",
    "            if similarity_set - similarity_sort >= 60:\n",
    "                similarity = (similarity_set * 0.6) + (similarity_sort * 0.4)\n",
    "            else:\n",
    "                similarity = (similarity_sort * 0.6) + (similarity_set * 0.4)\n",
    "\n",
    "            #jaccard smilarity\n",
    "            intersection = len(page1_keywords.intersection(page2_keywords))\n",
    "            union = len(page1_keywords.union(page2_keywords))\n",
    "            jaccard_similarity = intersection / union if union > 0 else 0\n",
    "            combined_score = (0.7 * similarity) + (0.3 * jaccard_similarity * 100) #combined smilarity\n",
    "\n",
    "            if jaccard_similarity > 0:\n",
    "                similarity_results.append({\n",
    "                    'Page 1': pages[i],\n",
    "                    'Page 2': pages[j],\n",
    "                    'Similarity (%)': int(similarity),\n",
    "                    'Jaccard Similarity (%)': int(jaccard_similarity * 100),\n",
    "                    'Combined Score (%)': int(combined_score)\n",
    "                })\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "similarity_df = pd.DataFrame(similarity_results)\n",
    "similarity_df = similarity_df[similarity_df['Similarity (%)'] >= 60]\n",
    "\n",
    "\n",
    "def find_top_keywords(page1, page2):\n",
    "\n",
    "    page1_keywords = search_console_df[search_console_df['Page'] == page1].groupby('Keyword').agg({'Clicks': 'sum', 'Impressions': 'sum'})\n",
    "    page2_keywords = search_console_df[search_console_df['Page'] == page2].groupby('Keyword').agg({'Clicks': 'sum', 'Impressions': 'sum'})\n",
    "\n",
    "\n",
    "    common_keywords = list(set(page1_keywords.index) & set(page2_keywords.index))\n",
    "\n",
    "    if common_keywords:\n",
    " \n",
    "        keyword_data = pd.concat([page1_keywords.loc[common_keywords], page2_keywords.loc[common_keywords]])\n",
    "        \n",
    "\n",
    "        filtered_keyword_data = keyword_data[(keyword_data['Clicks'] > 0).groupby(keyword_data.index).transform('all')]\n",
    "        \n",
    "\n",
    "        keyword_counts = filtered_keyword_data.groupby('Keyword').agg({'Clicks': 'sum', 'Impressions': 'sum'})\n",
    "\n",
    "\n",
    "        top_clicks_keyword = keyword_counts['Clicks'].idxmax() if not keyword_counts.empty else None\n",
    "        top_clicks_count = keyword_counts['Clicks'].max() if not keyword_counts.empty else None\n",
    "\n",
    "\n",
    "        top_impressions_keyword = keyword_counts['Impressions'].idxmax() if not keyword_counts.empty else None\n",
    "        top_impressions_count = keyword_counts['Impressions'].max() if not keyword_counts.empty else None\n",
    "\n",
    "        return top_clicks_keyword, top_clicks_count, top_impressions_keyword, top_impressions_count\n",
    "    else:\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "similarity_df['Top Clicks Keyword'] = None\n",
    "similarity_df['Top Clicks Count'] = None\n",
    "similarity_df['Top Impressions Keyword'] = None\n",
    "similarity_df['Top Impressions Count'] = None\n",
    "\n",
    "for index, row in similarity_df.iterrows():\n",
    "    top_clicks_keyword, top_clicks_count, top_impressions_keyword, top_impressions_count = \\\n",
    "        find_top_keywords(row['Page 1'], row['Page 2'])\n",
    "\n",
    "    similarity_df.loc[index, 'Top Clicks Keyword'] = top_clicks_keyword\n",
    "    similarity_df.loc[index, 'Top Clicks Count'] = top_clicks_count\n",
    "    similarity_df.loc[index, 'Top Impressions Keyword'] = top_impressions_keyword\n",
    "    similarity_df.loc[index, 'Top Impressions Count'] = top_impressions_count\n",
    "\n",
    "print(similarity_df)\n",
    "similarity_df.to_csv(\"final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
